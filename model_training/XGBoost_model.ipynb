{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model using XGBoost\n",
    "\n",
    "This notebook explores the following techniques to improve the performance of the XGBoost model in detecting fraudulent job postings:\n",
    "\n",
    "- **Various `scale_pos_weight`:** Adjusts the balance between positive (fraudulent) and negative (non-fraudulent) weights to handle the class imbalance in the dataset.\n",
    "\n",
    "- **Threshold Adjustment:** Helps to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent.\n",
    "\n",
    "- **Stratified K-Fold Cross-Validation:** Ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the original dataset, providing a more reliable evaluation.\n",
    "\n",
    "- **Hyperparameter Tuning:** Used to find the optimal set of model parameters (such as learning rate, tree depth, and number of estimators) to improve overall model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data_cleaned_3.csv') ##need to replace with ur own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_employment_type</th>\n",
       "      <th>has_required_experience</th>\n",
       "      <th>has_required_education</th>\n",
       "      <th>has_industry</th>\n",
       "      <th>...</th>\n",
       "      <th>city_ wilmington</th>\n",
       "      <th>city_ woodbridge</th>\n",
       "      <th>city_ woodruff</th>\n",
       "      <th>city_ worcester</th>\n",
       "      <th>city_ İstanbul</th>\n",
       "      <th>city_ Αthens</th>\n",
       "      <th>city_ Αθήνα</th>\n",
       "      <th>city_ ΕΛΛΗΝΙΚΟ</th>\n",
       "      <th>city_ 마포구 동교동</th>\n",
       "      <th>city_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14818 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id  telecommuting  has_company_logo  has_questions  fraudulent  \\\n",
       "0       1              0                 1              0           0   \n",
       "1       2              0                 1              0           0   \n",
       "2       3              0                 1              0           0   \n",
       "3       4              0                 1              0           0   \n",
       "4       5              0                 1              1           0   \n",
       "\n",
       "   has_location  has_employment_type  has_required_experience  \\\n",
       "0             1                    1                        1   \n",
       "1             1                    1                        1   \n",
       "2             1                    0                        0   \n",
       "3             1                    1                        1   \n",
       "4             1                    1                        1   \n",
       "\n",
       "   has_required_education  has_industry  ...  city_ wilmington  \\\n",
       "0                       0             0  ...               0.0   \n",
       "1                       0             1  ...               0.0   \n",
       "2                       0             0  ...               0.0   \n",
       "3                       1             1  ...               0.0   \n",
       "4                       1             1  ...               0.0   \n",
       "\n",
       "   city_ woodbridge   city_ woodruff  city_ worcester  city_ İstanbul  \\\n",
       "0                0.0             0.0              0.0             0.0   \n",
       "1                0.0             0.0              0.0             0.0   \n",
       "2                0.0             0.0              0.0             0.0   \n",
       "3                0.0             0.0              0.0             0.0   \n",
       "4                0.0             0.0              0.0             0.0   \n",
       "\n",
       "   city_ Αthens  city_ Αθήνα  city_ ΕΛΛΗΝΙΚΟ  city_ 마포구 동교동  city_Unknown  \n",
       "0           0.0          0.0             0.0            0.0           0.0  \n",
       "1           0.0          0.0             0.0            0.0           0.0  \n",
       "2           0.0          0.0             0.0            0.0           0.0  \n",
       "3           0.0          0.0             0.0            0.0           0.0  \n",
       "4           0.0          0.0             0.0            0.0           0.0  \n",
       "\n",
       "[5 rows x 14818 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(df.select_dtypes(include=['object']).columns)\n",
    "numeric_features = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "if 'fraudulent' in numeric_features:\n",
    "    numeric_features.remove('fraudulent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 10728\n",
      "Validation set size: 3576\n",
      "Test set size: 3576\n"
     ]
    }
   ],
   "source": [
    "def create_preprocessor(categorical_features, numeric_features):\n",
    "    transformers = []\n",
    "\n",
    "    transformers.append(\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    )\n",
    "\n",
    "    # transformers.append(('scaler', StandardScaler(), numeric_features))\n",
    "    \n",
    "    # return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "X = pd.concat([df[categorical_features + numeric_features]], axis=1)\n",
    "y = df['fraudulent']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the train+validation set into separate train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base XGBoost Model\n",
    "\n",
    "**Key Techniques**:  \n",
    "Baseline XGBoost model for benchmarking.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "Our model might benefit from techniques that can deal with class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9890    0.9994    0.9942      3403\n",
      "           1     0.9854    0.7803    0.8710       173\n",
      "\n",
      "    accuracy                         0.9888      3576\n",
      "   macro avg     0.9872    0.8899    0.9326      3576\n",
      "weighted avg     0.9888    0.9888    0.9882      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3401    2]\n",
      " [  38  135]]\n",
      "ROC AUC Score: 0.9925\n",
      "AUPRC: 0.9537\n"
     ]
    }
   ],
   "source": [
    "# Build the XGBoost model\n",
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting scale_pos_weight in XGBoost\n",
    "\n",
    "**Key Techniques**:  \n",
    "A commonly used parameter in xgboost is scale_pos_weight which is used to control the balance of positive and negative weights, which is useful for imbalanced datasets. Setting this parameter will help the algorithm give more weights to the minority class during training.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "We can see that using different weights for the classes resulted in an improvement in AUPRC. Therefore, we will select the weight with the highest AUPRC, which is scale_pos_weight = 6.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics for Different scale_pos_weight Values:\n",
      "   scale_pos_weight   roc_auc     auprc\n",
      "0          0.800000  0.991865  0.946108\n",
      "1          1.000000  0.992496  0.953670\n",
      "2          2.000000  0.993282  0.951913\n",
      "3         19.630769  0.994175  0.944315\n",
      "4         58.892308  0.990146  0.943182\n",
      "\n",
      "Best scale_pos_weight based on AUPRC: 1.00 with AUPRC: 0.9537\n"
     ]
    }
   ],
   "source": [
    "# Define scale_pos_weight values to test\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "scale_pos_weights = [0.8, 1, 2, scale_pos_weight_base, scale_pos_weight_base * 3]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over different scale_pos_weight values\n",
    "for spw in scale_pos_weights:\n",
    "    \n",
    "    # Build the XGBoost model with the current scale_pos_weight\n",
    "    model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=spw)\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate and print the ROC AUC score\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    # Store the result\n",
    "    results.append({\n",
    "        'scale_pos_weight': spw,\n",
    "        'roc_auc': roc_auc,\n",
    "        'auprc': auprc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Different scale_pos_weight Values:\")\n",
    "print(df_results)\n",
    "\n",
    "# Find the best scale_pos_weight based on AUPRC\n",
    "best_spw = df_results.loc[df_results['auprc'].idxmax()]\n",
    "print(f\"\\nBest scale_pos_weight based on AUPRC: {best_spw['scale_pos_weight']:.2f} with AUPRC: {best_spw['auprc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "\n",
    "**Key Techniques**:  \n",
    "Undersampling is used to balance the dataset by reducing the number of majority class samples. This method helps ensure the model is trained on a dataset with a more even distribution between the positive and negative classes, allowing it to better recognize patterns related to the minority class.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "Most results of undersampling fall below the baseline model’s AUPRC. This could be a result of key information being removed from the original dataset, which causes the model to not be able to identify certain patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling strategy 0.1 - Resampled class distribution: Counter({0: 5200, 1: 520})\n",
      "Sampling strategy 0.2 - Resampled class distribution: Counter({0: 2600, 1: 520})\n",
      "Sampling strategy 0.3 - Resampled class distribution: Counter({0: 1733, 1: 520})\n",
      "Sampling strategy 0.4 - Resampled class distribution: Counter({0: 1300, 1: 520})\n",
      "Sampling strategy 0.5 - Resampled class distribution: Counter({0: 1040, 1: 520})\n",
      "Results for Different Sampling Strategies:\n",
      "    Pos Weight  Sampling Strategy   ROC AUC     AUPRC\n",
      "0     1.000000                0.1  0.991419  0.942159\n",
      "1    10.000000                0.1  0.994473  0.951148\n",
      "2    30.000000                0.1  0.992947  0.949394\n",
      "3    50.000000                0.1  0.993027  0.946117\n",
      "4     3.333333                0.1  0.994843  0.954789\n",
      "5     2.000000                0.1  0.994896  0.951590\n",
      "6     1.000000                0.2  0.992866  0.935690\n",
      "7     5.000000                0.2  0.991437  0.937829\n",
      "8    15.000000                0.2  0.992253  0.939776\n",
      "9    25.000000                0.2  0.992890  0.941044\n",
      "10    1.666667                0.2  0.993508  0.942966\n",
      "11    1.000000                0.2  0.992866  0.935690\n",
      "12    1.000000                0.3  0.986788  0.913767\n",
      "13    3.332692                0.3  0.990969  0.930255\n",
      "14    9.998077                0.3  0.989066  0.919439\n",
      "15   16.663462                0.3  0.992248  0.935813\n",
      "16    1.110897                0.3  0.990340  0.925635\n",
      "17    0.666538                0.3  0.989263  0.924626\n",
      "18    1.000000                0.4  0.989633  0.910712\n",
      "19    2.500000                0.4  0.989960  0.908925\n",
      "20    7.500000                0.4  0.991002  0.920850\n",
      "21   12.500000                0.4  0.991560  0.915482\n",
      "22    0.833333                0.4  0.991062  0.918380\n",
      "23    0.500000                0.4  0.989170  0.914108\n",
      "24    1.000000                0.5  0.990933  0.921283\n",
      "25    2.000000                0.5  0.990112  0.909779\n",
      "26    6.000000                0.5  0.990581  0.916470\n",
      "27   10.000000                0.5  0.988480  0.915307\n",
      "28    0.666667                0.5  0.990167  0.919000\n",
      "29    0.400000                0.5  0.989064  0.913965\n",
      "\n",
      "Best Sampling Strategy:\n",
      "Pos Weight           3.333333\n",
      "Sampling Strategy    0.100000\n",
      "ROC AUC              0.994843\n",
      "AUPRC                0.954789\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Define sampling strategies to test\n",
    "sampling_strategies = [0.1, 0.2, 0.3, 0.4, 0.5]  # Test different undersampling ratios\n",
    "results = []\n",
    "\n",
    "# Loop through each sampling strategy\n",
    "for strategy in sampling_strategies:\n",
    "    # Apply undersampling with the current strategy\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=strategy, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Check the class distribution after undersampling\n",
    "    print(f'Sampling strategy {strategy} - Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "    # Define scale_pos_weight values to test\n",
    "    counter = Counter(y_train_resampled)\n",
    "    scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "    scale_pos_weights = [1, scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base * 5, scale_pos_weight_base / 3, scale_pos_weight_base / 5]\n",
    "\n",
    "    for weight in scale_pos_weights:\n",
    "        # Build the XGBoost model\n",
    "        model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=weight)\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline on the undersampled data\n",
    "        pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Calculate the ROC AUC score\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Calculate the AUPRC (Area Under Precision-Recall Curve)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({'Pos Weight': weight, 'Sampling Strategy': strategy, 'ROC AUC': roc_auc, 'AUPRC': auprc})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results table\n",
    "print(\"Results for Different Sampling Strategies:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find and print the best sampling strategy based on AUPRC\n",
    "best_strategy = results_df.loc[results_df['AUPRC'].idxmax()]\n",
    "print(\"\\nBest Sampling Strategy:\")\n",
    "print(best_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "**Key Techniques**:  \n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is applied to increase the number of minority class samples by generating synthetic examples. This method is aimed to create a balanced dataset that could help the model better identify minority class patterns. Different sampling strategies were tested, ranging from 0.35 to 0.6, in combination with various scale_pos_weight values to find the best performance.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "The application of SMOTE showed improved results, which might indicate that the model is better at handling class imbalance now. The optimal performance was with a sampling strategy of 0.35 and a scale_pos_weight of 0.952, achieving an AUPRC of 0.9550 and ROC AUC of 0.9951. This is an improvement over previous iterations, showing that SMOTE effectively leveraged synthetic data to boost detection of the minority class. Higher sampling strategies, such as 0.6, led to a drop in AUPRC, indicating that oversampling beyond a certain point may result in diminishing returns or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling strategy 0.35 - Resampled class distribution: Counter({0: 10208, 1: 3572})\n",
      "Sampling strategy 0.4 - Resampled class distribution: Counter({0: 10208, 1: 4083})\n",
      "Sampling strategy 0.5 - Resampled class distribution: Counter({0: 10208, 1: 5104})\n",
      "Sampling strategy 0.6 - Resampled class distribution: Counter({0: 10208, 1: 6124})\n",
      "Results for Different Sampling Strategies:\n",
      "    Pos Weight  Sampling Strategy   ROC AUC     AUPRC\n",
      "0     1.000000               0.35  0.994310  0.952772\n",
      "1     2.857783               0.35  0.992721  0.944477\n",
      "2     8.573348               0.35  0.990187  0.946095\n",
      "3    14.288914               0.35  0.990921  0.951801\n",
      "4     0.952594               0.35  0.993090  0.946480\n",
      "5     0.571557               0.35  0.992336  0.943297\n",
      "6     1.000000               0.40  0.993814  0.948156\n",
      "7     2.500122               0.40  0.992348  0.946089\n",
      "8     7.500367               0.40  0.991218  0.944005\n",
      "9    12.500612               0.40  0.994091  0.950523\n",
      "10    0.833374               0.40  0.994233  0.952001\n",
      "11    0.500024               0.40  0.994242  0.950515\n",
      "12    1.000000               0.50  0.994294  0.955908\n",
      "13    2.000000               0.50  0.994333  0.953950\n",
      "14    6.000000               0.50  0.992310  0.949593\n",
      "15   10.000000               0.50  0.993571  0.952328\n",
      "16    0.666667               0.50  0.992538  0.949791\n",
      "17    0.400000               0.50  0.995011  0.950599\n",
      "18    1.000000               0.60  0.992383  0.946846\n",
      "19    1.666884               0.60  0.993961  0.953780\n",
      "20    5.000653               0.60  0.991981  0.943919\n",
      "21    8.334422               0.60  0.991848  0.942546\n",
      "22    0.555628               0.60  0.993987  0.955020\n",
      "23    0.333377               0.60  0.995436  0.952544\n",
      "\n",
      "Best Sampling Strategy:\n",
      "Pos Weight           1.000000\n",
      "Sampling Strategy    0.500000\n",
      "ROC AUC              0.994294\n",
      "AUPRC                0.955908\n",
      "Name: 12, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Define sampling strategies to test\n",
    "sampling_strategies = [0.35, 0.4, 0.5, 0.6]  # Proportions of the minority class after resampling\n",
    "results = []\n",
    "\n",
    "# Loop through each sampling strategy\n",
    "for strategy in sampling_strategies:\n",
    "    # Apply SMOTE with the current strategy\n",
    "    smote = SMOTE(sampling_strategy=strategy, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Check the class distribution after resampling\n",
    "    print(f'Sampling strategy {strategy} - Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "    # Define scale_pos_weight values to test\n",
    "    counter = Counter(y_train_resampled)\n",
    "    scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "    scale_pos_weights = [1, scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base * 5, scale_pos_weight_base / 3, scale_pos_weight_base / 5]\n",
    "\n",
    "    for weight in scale_pos_weights:\n",
    "        # Build the XGBoost model\n",
    "        model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=weight)\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline on the resampled data\n",
    "        pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Calculate the ROC AUC score\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Calculate the AUPRC (Area Under Precision-Recall Curve)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({'Pos Weight': weight, 'Sampling Strategy': strategy, 'ROC AUC': roc_auc, 'AUPRC': auprc})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results table\n",
    "print(\"Results for Different Sampling Strategies:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find and print the best sampling strategy based on AUPRC\n",
    "best_strategy = results_df.loc[results_df['AUPRC'].idxmax()]\n",
    "print(\"\\nBest Sampling Strategy:\")\n",
    "print(best_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: With SMOTE, Without pos_weight\n",
      "Cross-validated AUPRC Scores: [0.93149154 0.9236318  0.92359593 0.9325928  0.90405835]\n",
      "Mean AUPRC Score: 0.9231\n",
      "\n",
      "\n",
      "Experiment: Without SMOTE, Without pos_weight\n",
      "Cross-validated AUPRC Scores: [0.94132849 0.92487731 0.93334326 0.91785941 0.91944705]\n",
      "Mean AUPRC Score: 0.9274\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's Pipeline to integrate SMOTE\n",
    "\n",
    "def auprc_score(y_true, y_pred_proba):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Set up stratified k-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Experiment configurations\n",
    "experiments = [\n",
    "    {'use_smote': True, 'pos_weight': 1.0, 'description': 'With SMOTE, Without pos_weight'},\n",
    "    {'use_smote': False, 'pos_weight': 1.0, 'description': 'Without SMOTE, Without pos_weight'}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    # Build the XGBoost model with or without pos_weight\n",
    "    model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=experiment['pos_weight'])\n",
    "\n",
    "    # Create the pipeline with or without SMOTE\n",
    "    if experiment['use_smote']:\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('smote', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    # Run cross-validation\n",
    "    cv_scores_auprc = cross_val_score(pipeline, X_train, y_train, cv=kf, \n",
    "                                      scoring=make_scorer(auprc_score, needs_proba=True))\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Description': experiment['description'],\n",
    "        'AUPRC Scores': cv_scores_auprc,\n",
    "        'Mean AUPRC': np.mean(cv_scores_auprc)\n",
    "    })\n",
    "\n",
    "# Print results for each experiment\n",
    "for result in results:\n",
    "    print(f\"Experiment: {result['Description']}\")\n",
    "    print(f\"Cross-validated AUPRC Scores: {result['AUPRC Scores']}\")\n",
    "    print(f\"Mean AUPRC Score: {result['Mean AUPRC']:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation ROC AUC: 0.995655\n",
      "Validation AUPRC: 0.967228\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "best_sampling_strategy = 0.5\n",
    "best_pos_weight = 1\n",
    "\n",
    "smote = SMOTE(sampling_strategy=best_sampling_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=best_pos_weight)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined\n",
    "    ('model', model),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_val_pred_proba = pipeline.predict_proba(X_val)[:, 1]  # Get probabilities for the positive class\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_pred_proba)\n",
    "\n",
    "precision_val, recall_val, _ = precision_recall_curve(y_val, y_val_pred_proba)\n",
    "auprc_val = auc(recall_val, precision_val)\n",
    "\n",
    "print(f\"Validation ROC AUC: {roc_auc_val:.6f}\")\n",
    "print(f\"Validation AUPRC: {auprc_val:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
