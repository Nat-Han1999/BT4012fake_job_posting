{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model using XGBoost\n",
    "\n",
    "This notebook explores the following techniques to improve the performance of the XGBoost model in detecting fraudulent job postings:\n",
    "\n",
    "- **Various `scale_pos_weight`:** Adjusts the balance between positive (fraudulent) and negative (non-fraudulent) weights to handle the class imbalance in the dataset.\n",
    "\n",
    "- **Threshold Adjustment:** Helps to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent.\n",
    "\n",
    "- **Stratified K-Fold Cross-Validation:** Ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the original dataset, providing a more reliable evaluation.\n",
    "\n",
    "- **Hyperparameter Tuning:** Used to find the optimal set of model parameters (such as learning rate, tree depth, and number of estimators) to improve overall model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data_cleaned.csv') ##need to replace with ur own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>...</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>requirements_cleaned</th>\n",
       "      <th>benefits_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Sales Professionals</td>\n",
       "      <td>Sales/Marketing</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Do YOU have the sales skills or entrepreneuria...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>High School or equivalent</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>IN</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>sale skill entrepreneurial drive join usflexko...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medical Surgical RN</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Find more jobs at #URL_4708e598004bb0a85bf09f9...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>find job url_ebbabffeeccaffdcfedebeeffeapply l...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Mechanical Design Engineer</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Aker Solutions is a global provider of product...</td>\n",
       "      <td>Corporate overviewAker Solutions is a global p...</td>\n",
       "      <td>Qualifications &amp;amp; personal attributes :Degr...</td>\n",
       "      <td>We offer :• Friendly colleagues in an industry...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Oil &amp; Energy</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>Houston</td>\n",
       "      <td>corporate overviewaker solution global provide...</td>\n",
       "      <td>qualification amp personal attribute degree me...</td>\n",
       "      <td>offer friendly colleague industry bright futur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>franciscan st. francis health</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Apply using below link#URL_ff6a6560a6c8ffc9abc...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>IN</td>\n",
       "      <td>Indianapolis</td>\n",
       "      <td>apply using linkurl_ffaacffcabcecadddaddeddedf...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Director of Peri-Anesthesia</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Apply using below link directly#URL_af5a535903...</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>Not Provided</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>MA</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>apply using link directlyurl_afaaacceabcdfaeda...</td>\n",
       "      <td>not provided</td>\n",
       "      <td>not provided</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               title       department  salary_range  \\\n",
       "0         Senior Sales Professionals  Sales/Marketing  Not Provided   \n",
       "1               Medical Surgical RN      Not Provided  Not Provided   \n",
       "2  Senior Mechanical Design Engineer     Not Provided  Not Provided   \n",
       "3      franciscan st. francis health     Not Provided  Not Provided   \n",
       "4      Director of Peri-Anesthesia       Not Provided  Not Provided   \n",
       "\n",
       "                                     company_profile  \\\n",
       "0                                       Not Provided   \n",
       "1                                       Not Provided   \n",
       "2  Aker Solutions is a global provider of product...   \n",
       "3                                       Not Provided   \n",
       "4                                       Not Provided   \n",
       "\n",
       "                                         description  \\\n",
       "0  Do YOU have the sales skills or entrepreneuria...   \n",
       "1  Find more jobs at #URL_4708e598004bb0a85bf09f9...   \n",
       "2  Corporate overviewAker Solutions is a global p...   \n",
       "3  Apply using below link#URL_ff6a6560a6c8ffc9abc...   \n",
       "4  Apply using below link directly#URL_af5a535903...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0                                       Not Provided   \n",
       "1                                       Not Provided   \n",
       "2  Qualifications &amp; personal attributes :Degr...   \n",
       "3                                       Not Provided   \n",
       "4                                       Not Provided   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                       Not Provided              0   \n",
       "1                                       Not Provided              0   \n",
       "2  We offer :• Friendly colleagues in an industry...              0   \n",
       "3                                       Not Provided              0   \n",
       "4                                       Not Provided              0   \n",
       "\n",
       "   has_company_logo  has_questions  ...         required_education  \\\n",
       "0                 0              0  ...  High School or equivalent   \n",
       "1                 0              0  ...                    Unknown   \n",
       "2                 1              0  ...                    Unknown   \n",
       "3                 0              0  ...                    Unknown   \n",
       "4                 0              0  ...                    Unknown   \n",
       "\n",
       "                 industry     function fraudulent country  state  \\\n",
       "0                 Unknown        Sales          1      US     IN   \n",
       "1  Hospital & Health Care      Unknown          1      US     CA   \n",
       "2            Oil & Energy  Engineering          1      US     TX   \n",
       "3  Hospital & Health Care      Unknown          1      US     IN   \n",
       "4  Hospital & Health Care      Unknown          1      US     MA   \n",
       "\n",
       "            city                                description_cleaned  \\\n",
       "0   Indianapolis  sale skill entrepreneurial drive join usflexko...   \n",
       "1        Unknown  find job url_ebbabffeeccaffdcfedebeeffeapply l...   \n",
       "2        Houston  corporate overviewaker solution global provide...   \n",
       "3   Indianapolis  apply using linkurl_ffaacffcabcecadddaddeddedf...   \n",
       "4        Unknown  apply using link directlyurl_afaaacceabcdfaeda...   \n",
       "\n",
       "                                requirements_cleaned  \\\n",
       "0                                       not provided   \n",
       "1                                       not provided   \n",
       "2  qualification amp personal attribute degree me...   \n",
       "3                                       not provided   \n",
       "4                                       not provided   \n",
       "\n",
       "                                    benefits_cleaned  \n",
       "0                                       not provided  \n",
       "1                                       not provided  \n",
       "2  offer friendly colleague industry bright futur...  \n",
       "3                                       not provided  \n",
       "4                                       not provided  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\n",
    "    'description_cleaned', 'requirements_cleaned', 'benefits_cleaned', 'company_profile'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'title', 'department', 'employment_type', 'required_experience',\n",
    "    'required_education', 'industry', 'function', 'country', 'state', 'city'\n",
    "]\n",
    "\n",
    "numeric_features = ['telecommuting', 'has_company_logo', 'has_questions']\n",
    "\n",
    "y = df['fraudulent']\n",
    "\n",
    "df[text_features] = df[text_features].fillna('')\n",
    "\n",
    "# df[categorical_features] = df[categorical_features].fillna('Unknown')\n",
    "\n",
    "# Ensure numerical features have no missing values\n",
    "df[numeric_features] = df[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(text_features, categorical_features, numeric_features):\n",
    "    transformers = []\n",
    "\n",
    "    # Text features: Apply TfidfVectorizer to each text feature separately\n",
    "    for feature in text_features:\n",
    "        tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        transformers.append((f'tfidf_{feature}', tfidf, feature))\n",
    "\n",
    "    # Categorical features: Apply OneHotEncoder (returning dense output to avoid issues with XGBoost)\n",
    "    transformers.append(\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    )\n",
    "\n",
    "    # Numerical features: Pass through without changes\n",
    "    transformers.append(('passthrough', 'passthrough', numeric_features))\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = create_preprocessor(text_features, categorical_features, numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[text_features + categorical_features + numeric_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base XGBoost Model\n",
    "\n",
    "A base XGBoost model is used as a benchmark to identify further techniques needed to be applied to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9884    0.9974    0.9928      3403\n",
      "           1     0.9366    0.7688    0.8444       173\n",
      "\n",
      "    accuracy                         0.9863      3576\n",
      "   macro avg     0.9625    0.8831    0.9186      3576\n",
      "weighted avg     0.9858    0.9863    0.9857      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3394    9]\n",
      " [  40  133]]\n",
      "AUPRC: 0.9231\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42)\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting scale_pos_weight in XGBoost\n",
    "\n",
    "scale_pos_weight in XGBoost is used to control the balance of positive and negative weights, which is useful for imbalanced datasets. Setting this parameter will help the algorithm give more weights to the minority class during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Pos Weight: 19.64\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9944    0.9903    0.9923      3403\n",
      "           1     0.8235    0.8902    0.8556       173\n",
      "\n",
      "    accuracy                         0.9855      3576\n",
      "   macro avg     0.9090    0.9402    0.9239      3576\n",
      "weighted avg     0.9861    0.9855    0.9857      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3370   33]\n",
      " [  19  154]]\n",
      "AUPRC: 0.9302\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight = counter[0] / counter[1]\n",
    "\n",
    "print(f'Scale Pos Weight: {scale_pos_weight:.2f}')\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: \n",
    "\n",
    "There is an increased recall for the fraudulent class which means that the model is now detecting a higher percentage of actual fraudulent jobs. There is a decreased pecision for fraudulent class which means that the model is incorrectly labeling more non-fraudulent jobs as fraudulent.\n",
    "\n",
    "In business sense, we would have to spend more resources to vet through more reports because of the high false positive but we also have a higher detection rate of fraudlent job postings which might led to higher trust in our businesses. There is a trade of that could be quantified in business dollars - the risk of a fraudulent job versus the additional resources needed to vet the false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering a range of scale_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In imbalanced datasets like ours, experimenting with a range of scale_pos_weight values helps us balance the importance between the minority (fraudulent jobs) and majority (non-fraudulent jobs) classes, ensuring that the model doesn't become biased towards the majority class and improves its ability to detect fraudulent jobs (i.e., higher recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing scale_pos_weight = 1.00\n",
      "AUPRC: 0.9231\n",
      "\n",
      "Testing scale_pos_weight = 19.64\n",
      "AUPRC: 0.9302\n",
      "\n",
      "Testing scale_pos_weight = 23.57\n",
      "AUPRC: 0.9309\n",
      "\n",
      "Testing scale_pos_weight = 29.46\n",
      "AUPRC: 0.9213\n",
      "\n",
      "Testing scale_pos_weight = 39.28\n",
      "AUPRC: 0.9196\n",
      "\n",
      "Testing scale_pos_weight = 58.92\n",
      "AUPRC: 0.9090\n",
      "\n",
      "Evaluation Metrics (AUPRC) for Different scale_pos_weight Values:\n",
      "   scale_pos_weight     aucpr\n",
      "0          1.000000  0.923075\n",
      "1         19.640693  0.930178\n",
      "2         23.568831  0.930896\n",
      "3         29.461039  0.921295\n",
      "4         39.281385  0.919640\n",
      "5         58.922078  0.909044\n",
      "\n",
      "Best scale_pos_weight based on AUPRC: 23.57 with AUPRC: 0.9309\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "scale_pos_weights = [1, scale_pos_weight_base, scale_pos_weight_base * 1.2, \n",
    "                     scale_pos_weight_base * 1.5, scale_pos_weight_base * 2, scale_pos_weight_base * 3]\n",
    "\n",
    "results = []\n",
    "\n",
    "for spw in scale_pos_weights:\n",
    "    print(f\"\\nTesting scale_pos_weight = {spw:.2f}\")\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    aucpr = auc(recall, precision)\n",
    "    \n",
    "    results.append({\n",
    "        'scale_pos_weight': spw,\n",
    "        'aucpr': aucpr\n",
    "    })\n",
    "    \n",
    "    print(f\"AUPRC: {aucpr:.4f}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nEvaluation Metrics (AUPRC) for Different scale_pos_weight Values:\")\n",
    "print(df_results)\n",
    "\n",
    "best_spw = df_results.loc[df_results['aucpr'].idxmax()]\n",
    "print(f\"\\nBest scale_pos_weight based on AUPRC: {best_spw['scale_pos_weight']:.2f} with AUPRC: {best_spw['aucpr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Classification Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above experiement, we observed that the peak in AUCPR at approximately 0.9309 when scale_pos_weight is 23.57.\n",
    "\n",
    "Another possible action is to adjsut the classification threshold. Instead of using the default probability threshold of 0.5, we can experiment with different thresholds to balance precision and recall. This would help to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3, Precision: 0.5993, Recall: 0.9249, F1-Score: 0.7273\n",
      "Threshold: 0.4, Precision: 0.6569, Recall: 0.9075, F1-Score: 0.7621\n",
      "Threshold: 0.5, Precision: 0.6909, Recall: 0.8786, F1-Score: 0.7735\n",
      "Threshold: 0.6, Precision: 0.7449, Recall: 0.8439, F1-Score: 0.7913\n",
      "Threshold: 0.7, Precision: 0.8161, Recall: 0.8208, F1-Score: 0.8184\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in thresholds:\n",
    "    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)\n",
    "    report = classification_report(y_test, y_pred_adjusted, output_dict=True, digits=4)\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    print(f\"Threshold: {threshold}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiement on threshold above, we can see that as threshold increases, precision increases which means that fewer non-fraudulent jobs are incorrectly classified as fraudulent.\n",
    "\n",
    "Recall however, decreases which means that more fraudulent jobs are missed.\n",
    "\n",
    "As we need to find a balance between precision and recall, assuming that the risk and reward is equal, we will identify the point where the F1-Score peaks, which is when the threshold is 0.7.\n",
    "\n",
    "Area under the precision-recall curve is not used here as it is independent of the threshold number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration both of our findings from the scale_pos_weight and threshold experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report at Threshold 0.5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    0.9885    0.9917      3403\n",
      "           1     0.8000    0.9017    0.8478       173\n",
      "\n",
      "    accuracy                         0.9843      3576\n",
      "   macro avg     0.8975    0.9451    0.9198      3576\n",
      "weighted avg     0.9855    0.9843    0.9848      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.5:\n",
      "[[3364   39]\n",
      " [  17  156]]\n",
      "AUPRC: 0.9309\n",
      "Classification Report at Threshold 0.7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9918    0.9953    0.9935      3403\n",
      "           1     0.9006    0.8382    0.8683       173\n",
      "\n",
      "    accuracy                         0.9877      3576\n",
      "   macro avg     0.9462    0.9167    0.9309      3576\n",
      "weighted avg     0.9874    0.9877    0.9875      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.7:\n",
      "[[3387   16]\n",
      " [  28  145]]\n",
      "AUPRC: 0.9309\n"
     ]
    }
   ],
   "source": [
    "# Optimal scale_pos_weight\n",
    "optimal_scale_pos_weight = 23.57\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=optimal_scale_pos_weight,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Previous threshold of 0.5\n",
    "optimal_threshold = 0.5\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Classification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')  \n",
    "\n",
    "# Optimal threshold of 0.7\n",
    "optimal_threshold = 0.7\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Classification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the optimal threshold improves the f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using StratifiedKFold ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the entire dataset. Doing so will help provide more reliable evaluation for our highly imbalanced data and preventing bias towards the majority class during model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Scale Pos Weight for Fold 1: 23.57\n",
      "AUPRC for Fold 1: 0.9313\n",
      "\n",
      "Fold 2/5\n",
      "Scale Pos Weight for Fold 2: 23.57\n",
      "AUPRC for Fold 2: 0.9079\n",
      "\n",
      "Fold 3/5\n",
      "Scale Pos Weight for Fold 3: 23.57\n",
      "AUPRC for Fold 3: 0.9375\n",
      "\n",
      "Fold 4/5\n",
      "Scale Pos Weight for Fold 4: 23.57\n",
      "AUPRC for Fold 4: 0.9329\n",
      "\n",
      "Fold 5/5\n",
      "Scale Pos Weight for Fold 5: 23.57\n",
      "AUPRC for Fold 5: 0.9172\n",
      "\n",
      "Average AUPRC across all folds: 0.9254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "aucpr_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    scale_pos_weight_fold = 23.57\n",
    "    print(f\"Scale Pos Weight for Fold {fold + 1}: {scale_pos_weight_fold:.2f}\")\n",
    "    \n",
    "    model_fold = xgb.XGBClassifier(\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight_fold\n",
    "    )\n",
    "    \n",
    "    pipeline_fold = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_fold)\n",
    "    ])\n",
    "    \n",
    "    pipeline_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_proba_fold = pipeline_fold.predict_proba(X_test_fold)[:, 1]\n",
    "    \n",
    "    # Calculate Precision-Recall curve and AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_pred_proba_fold)\n",
    "    aucpr = auc(recall, precision)\n",
    "    aucpr_scores.append(aucpr)\n",
    "    \n",
    "    print(f\"AUPRC for Fold {fold + 1}: {aucpr:.4f}\")\n",
    "\n",
    "# Calculate and print average AUPRC across all folds\n",
    "avg_aucpr = np.mean(aucpr_scores)\n",
    "print(f\"\\nAverage AUPRC across all folds: {avg_aucpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SMOTE for imbalanced dataset\n",
    "\n",
    "We are considering to use SMOTE for our dataset. However, our data is high-dimensional because of the TF-IDF. SMOTE may not be as effective in such spaces because it relies on computing nearest neighbors, which can be unreliable in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "In this section, we intend to try out the different hyperparameters to test out if other parameters could improve our AUPRC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code takes 4 hours to run. Please take note before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Scale Pos Weight: 23.57\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters:\n",
      "{'model__n_estimators': 300, 'model__max_depth': 7, 'model__learning_rate': 0.1, 'model__gamma': 0.1}\n",
      "Best AUPRC Score from Cross-Validation: 0.9117\n",
      "\n",
      "Classification Report at Threshold 0.7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9962    0.9936      3403\n",
      "           1     0.9161    0.8208    0.8659       173\n",
      "\n",
      "    accuracy                         0.9877      3576\n",
      "   macro avg     0.9535    0.9085    0.9297      3576\n",
      "weighted avg     0.9873    0.9877    0.9874      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.7:\n",
      "[[3390   13]\n",
      " [  31  142]]\n"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = 23.57\n",
    "print(f\"Global Scale Pos Weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__gamma': [0, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',  \n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom scoring function to calculate AUPRC\n",
    "def custom_auprc(estimator, X, y_true):\n",
    "    y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Use custom_auprc without make_scorer (direct function)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring=custom_auprc,  # Custom AUPRC function without make_scorer\n",
    "    cv=skf,\n",
    "    n_jobs=1,  # Run without parallelization to avoid pickling issues\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best AUPRC Score from Cross-Validation: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best pipeline after hyperparameter tuning\n",
    "best_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Apply the optimal threshold for final predictions\n",
    "optimal_threshold = 0.7\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(f\"\\nClassification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
