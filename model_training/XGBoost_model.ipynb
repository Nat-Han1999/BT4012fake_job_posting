{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model using XGBoost\n",
    "\n",
    "This notebook explores the following techniques to improve the performance of the XGBoost model in detecting fraudulent job postings:\n",
    "\n",
    "- **Various `scale_pos_weight`:** Adjusts the balance between positive (fraudulent) and negative (non-fraudulent) weights to handle the class imbalance in the dataset.\n",
    "\n",
    "- **Threshold Adjustment:** Helps to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent.\n",
    "\n",
    "- **Stratified K-Fold Cross-Validation:** Ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the original dataset, providing a more reliable evaluation.\n",
    "\n",
    "- **Hyperparameter Tuning:** Used to find the optimal set of model parameters (such as learning rate, tree depth, and number of estimators) to improve overall model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data_cleaned_3.csv') ##need to replace with ur own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_employment_type</th>\n",
       "      <th>has_required_experience</th>\n",
       "      <th>has_required_education</th>\n",
       "      <th>has_industry</th>\n",
       "      <th>has_function</th>\n",
       "      <th>...</th>\n",
       "      <th>city_ wilmington</th>\n",
       "      <th>city_ woodbridge</th>\n",
       "      <th>city_ woodruff</th>\n",
       "      <th>city_ worcester</th>\n",
       "      <th>city_ İstanbul</th>\n",
       "      <th>city_ Αthens</th>\n",
       "      <th>city_ Αθήνα</th>\n",
       "      <th>city_ ΕΛΛΗΝΙΚΟ</th>\n",
       "      <th>city_ 마포구 동교동</th>\n",
       "      <th>city_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14817 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   telecommuting  has_company_logo  has_questions  fraudulent  has_location  \\\n",
       "0              0                 1              0           0             1   \n",
       "1              0                 1              0           0             1   \n",
       "2              0                 1              0           0             1   \n",
       "3              0                 1              0           0             1   \n",
       "4              0                 1              1           0             1   \n",
       "\n",
       "   has_employment_type  has_required_experience  has_required_education  \\\n",
       "0                    1                        1                       0   \n",
       "1                    1                        1                       0   \n",
       "2                    0                        0                       0   \n",
       "3                    1                        1                       1   \n",
       "4                    1                        1                       1   \n",
       "\n",
       "   has_industry  has_function  ...  city_ wilmington  city_ woodbridge   \\\n",
       "0             0             1  ...               0.0                0.0   \n",
       "1             1             1  ...               0.0                0.0   \n",
       "2             0             0  ...               0.0                0.0   \n",
       "3             1             1  ...               0.0                0.0   \n",
       "4             1             1  ...               0.0                0.0   \n",
       "\n",
       "   city_ woodruff  city_ worcester  city_ İstanbul  city_ Αthens  city_ Αθήνα  \\\n",
       "0             0.0              0.0             0.0           0.0          0.0   \n",
       "1             0.0              0.0             0.0           0.0          0.0   \n",
       "2             0.0              0.0             0.0           0.0          0.0   \n",
       "3             0.0              0.0             0.0           0.0          0.0   \n",
       "4             0.0              0.0             0.0           0.0          0.0   \n",
       "\n",
       "   city_ ΕΛΛΗΝΙΚΟ  city_ 마포구 동교동  city_Unknown  \n",
       "0             0.0            0.0           0.0  \n",
       "1             0.0            0.0           0.0  \n",
       "2             0.0            0.0           0.0  \n",
       "3             0.0            0.0           0.0  \n",
       "4             0.0            0.0           0.0  \n",
       "\n",
       "[5 rows x 14817 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(df.select_dtypes(include=['object']).columns)\n",
    "numeric_features = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "if 'fraudulent' in numeric_features:\n",
    "    numeric_features.remove('fraudulent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 10728\n",
      "Validation set size: 3576\n",
      "Test set size: 3576\n"
     ]
    }
   ],
   "source": [
    "def create_preprocessor(categorical_features, numeric_features):\n",
    "    transformers = []\n",
    "\n",
    "    transformers.append(\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    )\n",
    "\n",
    "    # transformers.append(('scaler', StandardScaler(), numeric_features))\n",
    "    \n",
    "    # return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "X = pd.concat([df[categorical_features + numeric_features]], axis=1)\n",
    "y = df['fraudulent']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Split the train+validation set into separate train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base XGBoost Model\n",
    "\n",
    "**Key Techniques**:  \n",
    "Baseline XGBoost model for benchmarking.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "Our model might benefit from techniques that can deal with class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9889    0.9991    0.9940      3403\n",
      "           1     0.9783    0.7803    0.8682       173\n",
      "\n",
      "    accuracy                         0.9885      3576\n",
      "   macro avg     0.9836    0.8897    0.9311      3576\n",
      "weighted avg     0.9884    0.9885    0.9879      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3400    3]\n",
      " [  38  135]]\n",
      "ROC AUC Score: 0.9937\n",
      "AUPRC: 0.9396\n"
     ]
    }
   ],
   "source": [
    "# Build the XGBoost model\n",
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting scale_pos_weight in XGBoost\n",
    "\n",
    "**Key Techniques**:  \n",
    "A commonly used parameter in xgboost is scale_pos_weight which is used to control the balance of positive and negative weights, which is useful for imbalanced datasets. Setting this parameter will help the algorithm give more weights to the minority class during training.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "We can see that using different weights for the classes resulted in an improvement in AUPRC. Therefore, we will select the weight with the highest AUPRC, which is scale_pos_weight = 6.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics for Different scale_pos_weight Values:\n",
      "   scale_pos_weight   roc_auc     auprc\n",
      "0         19.630769  0.990952  0.928323\n",
      "1         58.892308  0.985409  0.912075\n",
      "2         16.358974  0.987760  0.929887\n",
      "3          6.543590  0.989695  0.928984\n",
      "4          3.926154  0.990146  0.933261\n",
      "5          1.000000  0.993679  0.939606\n",
      "6          0.800000  0.992477  0.939251\n",
      "\n",
      "Best scale_pos_weight based on AUPRC: 1.00 with AUPRC: 0.9396\n"
     ]
    }
   ],
   "source": [
    "# Define scale_pos_weight values to test\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "scale_pos_weights = [scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base / 1.2, scale_pos_weight_base / 3, scale_pos_weight_base / 5, 1, 0.8]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over different scale_pos_weight values\n",
    "for spw in scale_pos_weights:\n",
    "    \n",
    "    # Build the XGBoost model with the current scale_pos_weight\n",
    "    model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=spw)\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate and print the ROC AUC score\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    \n",
    "    # Store the result\n",
    "    results.append({\n",
    "        'scale_pos_weight': spw,\n",
    "        'roc_auc': roc_auc,\n",
    "        'auprc': auprc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Different scale_pos_weight Values:\")\n",
    "print(df_results)\n",
    "\n",
    "# Find the best scale_pos_weight based on AUPRC\n",
    "best_spw = df_results.loc[df_results['auprc'].idxmax()]\n",
    "print(f\"\\nBest scale_pos_weight based on AUPRC: {best_spw['scale_pos_weight']:.2f} with AUPRC: {best_spw['auprc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Number of features in XGBoost\n",
    "\n",
    "**Key Techniques**:  \n",
    "A commonly\n",
    "\n",
    "**Key Discoveries**:  \n",
    "We can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     17\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m     18\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_selection\u001b[39m\u001b[38;5;124m'\u001b[39m, rfe),\n\u001b[1;32m     19\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[1;32m     20\u001b[0m ])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Train the pipeline\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/pipeline.py:329\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03mFit all the transforms/samplers one after the other and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    This estimator.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    328\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 329\u001b[0m Xt, yt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/pipeline.py:255\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    253\u001b[0m     cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m ):\n\u001b[0;32m--> 255\u001b[0m     X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cloned_transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_resample\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    265\u001b[0m     X, y, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_resample_one_cached(\n\u001b[1;32m    266\u001b[0m         cloned_transformer,\n\u001b[1;32m    267\u001b[0m         X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m         params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    272\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/joblib/memory.py:312\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/pipeline.py:1104\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1104\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1107\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1108\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1101\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:268\u001b[0m, in \u001b[0;36mRFE.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m--> 268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/feature_selection/_rfe.py:323\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[0;32m--> 323\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[1;32m    326\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[1;32m    327\u001b[0m     estimator,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[1;32m    329\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    330\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Sort by AUPRC in descending order\n",
    "sorted_df = df_results.sort_values(by='auprc', ascending=False)\n",
    "\n",
    "# Extract the scale_pos_weight corresponding to the best AUPRC\n",
    "best_pos_weight = sorted_df.iloc[0]['scale_pos_weight']\n",
    "\n",
    "# Build the XGBoost model\n",
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42)\n",
    "\n",
    "# Add RFE for feature selection\n",
    "rfe = RFE(estimator=model, n_features_to_select=10, step=1)  # Adjust `n_features_to_select` as needed\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', rfe),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling\n",
    "\n",
    "**Key Techniques**:  \n",
    "Undersampling is used to balance the dataset by reducing the number of majority class samples. This method helps ensure the model is trained on a dataset with a more even distribution between the positive and negative classes, allowing it to better recognize patterns related to the minority class.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "Most results of undersampling fall below the baseline model’s AUPRC. This could be a result of key information being removed from the original dataset, which causes the model to not be able to identify certain patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling strategy 0.1 - Resampled class distribution: Counter({0: 5200, 1: 520})\n",
      "Sampling strategy 0.2 - Resampled class distribution: Counter({0: 2600, 1: 520})\n",
      "Sampling strategy 0.3 - Resampled class distribution: Counter({0: 1733, 1: 520})\n",
      "Sampling strategy 0.4 - Resampled class distribution: Counter({0: 1300, 1: 520})\n",
      "Sampling strategy 0.5 - Resampled class distribution: Counter({0: 1040, 1: 520})\n",
      "Results for Different Sampling Strategies:\n",
      "    Pos Weight  Sampling Strategy   ROC AUC     AUPRC\n",
      "0     1.000000                0.1  0.991626  0.933507\n",
      "1    10.000000                0.1  0.990963  0.930573\n",
      "2    30.000000                0.1  0.986156  0.921582\n",
      "3    50.000000                0.1  0.989727  0.923002\n",
      "4     3.333333                0.1  0.994313  0.945419\n",
      "5     2.000000                0.1  0.990259  0.926018\n",
      "6     1.000000                0.2  0.992414  0.927712\n",
      "7     5.000000                0.2  0.989117  0.916327\n",
      "8    15.000000                0.2  0.990383  0.929958\n",
      "9    25.000000                0.2  0.989790  0.919522\n",
      "10    1.666667                0.2  0.991473  0.926782\n",
      "11    1.000000                0.2  0.992414  0.927712\n",
      "12    1.000000                0.3  0.987704  0.901888\n",
      "13    3.332692                0.3  0.990586  0.915088\n",
      "14    9.998077                0.3  0.989153  0.905105\n",
      "15   16.663462                0.3  0.989470  0.898445\n",
      "16    1.110897                0.3  0.989895  0.901978\n",
      "17    0.666538                0.3  0.988681  0.895160\n",
      "18    1.000000                0.4  0.988179  0.888265\n",
      "19    2.500000                0.4  0.988016  0.881985\n",
      "20    7.500000                0.4  0.986350  0.885320\n",
      "21   12.500000                0.4  0.990445  0.897052\n",
      "22    0.833333                0.4  0.987322  0.876015\n",
      "23    0.500000                0.4  0.986661  0.878273\n",
      "24    1.000000                0.5  0.986272  0.879343\n",
      "25    2.000000                0.5  0.986997  0.881684\n",
      "26    6.000000                0.5  0.987756  0.891025\n",
      "27   10.000000                0.5  0.988794  0.883734\n",
      "28    0.666667                0.5  0.986173  0.875074\n",
      "29    0.400000                0.5  0.984803  0.875595\n",
      "\n",
      "Best Sampling Strategy:\n",
      "Pos Weight           3.333333\n",
      "Sampling Strategy    0.100000\n",
      "ROC AUC              0.994313\n",
      "AUPRC                0.945419\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Define sampling strategies to test\n",
    "sampling_strategies = [0.1, 0.2, 0.3, 0.4, 0.5]  # Test different undersampling ratios\n",
    "results = []\n",
    "\n",
    "# Loop through each sampling strategy\n",
    "for strategy in sampling_strategies:\n",
    "    # Apply undersampling with the current strategy\n",
    "    under_sampler = RandomUnderSampler(sampling_strategy=strategy, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Check the class distribution after undersampling\n",
    "    print(f'Sampling strategy {strategy} - Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "    # Define scale_pos_weight values to test\n",
    "    counter = Counter(y_train_resampled)\n",
    "    scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "    scale_pos_weights = [1, scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base * 5, scale_pos_weight_base / 3, scale_pos_weight_base / 5]\n",
    "\n",
    "    for weight in scale_pos_weights:\n",
    "        # Build the XGBoost model\n",
    "        model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=weight)\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline on the undersampled data\n",
    "        pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Calculate the ROC AUC score\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Calculate the AUPRC (Area Under Precision-Recall Curve)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({'Pos Weight': weight, 'Sampling Strategy': strategy, 'ROC AUC': roc_auc, 'AUPRC': auprc})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results table\n",
    "print(\"Results for Different Sampling Strategies:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find and print the best sampling strategy based on AUPRC\n",
    "best_strategy = results_df.loc[results_df['AUPRC'].idxmax()]\n",
    "print(\"\\nBest Sampling Strategy:\")\n",
    "print(best_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "**Key Techniques**:  \n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is applied to increase the number of minority class samples by generating synthetic examples. This method is aimed to create a balanced dataset that could help the model better identify minority class patterns. Different sampling strategies were tested, ranging from 0.35 to 0.6, in combination with various scale_pos_weight values to find the best performance.\n",
    "\n",
    "**Key Discoveries**:  \n",
    "The application of SMOTE showed improved results, which might indicate that the model is better at handling class imbalance now. The optimal performance was with a sampling strategy of 0.35 and a scale_pos_weight of 0.952, achieving an AUPRC of 0.9550 and ROC AUC of 0.9951. This is an improvement over previous iterations, showing that SMOTE effectively leveraged synthetic data to boost detection of the minority class. Higher sampling strategies, such as 0.6, led to a drop in AUPRC, indicating that oversampling beyond a certain point may result in diminishing returns or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling strategy 0.35 - Resampled class distribution: Counter({0: 10208, 1: 3572})\n",
      "Sampling strategy 0.4 - Resampled class distribution: Counter({0: 10208, 1: 4083})\n",
      "Sampling strategy 0.5 - Resampled class distribution: Counter({0: 10208, 1: 5104})\n",
      "Sampling strategy 0.6 - Resampled class distribution: Counter({0: 10208, 1: 6124})\n",
      "Results for Different Sampling Strategies:\n",
      "    Pos Weight  Sampling Strategy   ROC AUC     AUPRC\n",
      "0     1.000000               0.35  0.991723  0.933790\n",
      "1     2.857783               0.35  0.991859  0.935113\n",
      "2     8.573348               0.35  0.990411  0.931808\n",
      "3    14.288914               0.35  0.986870  0.923884\n",
      "4     0.952594               0.35  0.990537  0.939818\n",
      "5     0.571557               0.35  0.990676  0.932854\n",
      "6     1.000000               0.40  0.989948  0.931335\n",
      "7     2.500122               0.40  0.991108  0.938165\n",
      "8     7.500367               0.40  0.985716  0.926718\n",
      "9    12.500612               0.40  0.988011  0.924114\n",
      "10    0.833374               0.40  0.987422  0.925023\n",
      "11    0.500024               0.40  0.990552  0.926205\n",
      "12    1.000000               0.50  0.989535  0.931890\n",
      "13    2.000000               0.50  0.989278  0.926269\n",
      "14    6.000000               0.50  0.986941  0.918154\n",
      "15   10.000000               0.50  0.986980  0.927925\n",
      "16    0.666667               0.50  0.988482  0.930851\n",
      "17    0.400000               0.50  0.987967  0.927830\n",
      "18    1.000000               0.60  0.990168  0.923585\n",
      "19    1.666884               0.60  0.989735  0.930423\n",
      "20    5.000653               0.60  0.990350  0.927178\n",
      "21    8.334422               0.60  0.986192  0.923563\n",
      "22    0.555628               0.60  0.985508  0.922379\n",
      "23    0.333377               0.60  0.987082  0.919822\n",
      "\n",
      "Best Sampling Strategy:\n",
      "Pos Weight           0.952594\n",
      "Sampling Strategy    0.350000\n",
      "ROC AUC              0.990537\n",
      "AUPRC                0.939818\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Define sampling strategies to test\n",
    "sampling_strategies = [0.35, 0.4, 0.5, 0.6]  # Proportions of the minority class after resampling\n",
    "results = []\n",
    "\n",
    "# Loop through each sampling strategy\n",
    "for strategy in sampling_strategies:\n",
    "    # Apply SMOTE with the current strategy\n",
    "    smote = SMOTE(sampling_strategy=strategy, random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Check the class distribution after resampling\n",
    "    print(f'Sampling strategy {strategy} - Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "    # Define scale_pos_weight values to test\n",
    "    counter = Counter(y_train_resampled)\n",
    "    scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "    scale_pos_weights = [1, scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base * 5, scale_pos_weight_base / 3, scale_pos_weight_base / 5]\n",
    "\n",
    "    for weight in scale_pos_weights:\n",
    "        # Build the XGBoost model\n",
    "        model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=weight)\n",
    "\n",
    "        # Create the pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "        # Train the pipeline on the resampled data\n",
    "        pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Calculate the ROC AUC score\n",
    "        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        # Calculate the AUPRC (Area Under Precision-Recall Curve)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        # Store the results\n",
    "        results.append({'Pos Weight': weight, 'Sampling Strategy': strategy, 'ROC AUC': roc_auc, 'AUPRC': auprc})\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Print the results table\n",
    "print(\"Results for Different Sampling Strategies:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find and print the best sampling strategy based on AUPRC\n",
    "best_strategy = results_df.loc[results_df['AUPRC'].idxmax()]\n",
    "print(\"\\nBest Sampling Strategy:\")\n",
    "print(best_strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: With SMOTE, With pos_weight\n",
      "Cross-validated AUPRC Scores: [0.90038339 0.90533844 0.88731215 0.8997315  0.9086766 ]\n",
      "Mean AUPRC Score: 0.9003\n",
      "\n",
      "\n",
      "Experiment: With SMOTE, Without pos_weight\n",
      "Cross-validated AUPRC Scores: [0.9069786  0.89489319 0.87268701 0.89308638 0.90312235]\n",
      "Mean AUPRC Score: 0.8942\n",
      "\n",
      "\n",
      "Experiment: Without SMOTE, With pos_weight\n",
      "Cross-validated AUPRC Scores: [0.90923503 0.88796294 0.87372273 0.8898812  0.9087252 ]\n",
      "Mean AUPRC Score: 0.8939\n",
      "\n",
      "\n",
      "Experiment: Without SMOTE, Without pos_weight\n",
      "Cross-validated AUPRC Scores: [0.90644182 0.90159239 0.87257346 0.89446309 0.91004514]\n",
      "Mean AUPRC Score: 0.8970\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, precision_recall_curve, auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn's Pipeline to integrate SMOTE\n",
    "\n",
    "def auprc_score(y_true, y_pred_proba):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Set up stratified k-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Experiment configurations\n",
    "experiments = [\n",
    "    {'use_smote': True, 'pos_weight': 0.952551, 'description': 'With SMOTE, With pos_weight'},\n",
    "    {'use_smote': True, 'pos_weight': 1.0, 'description': 'With SMOTE, Without pos_weight'},\n",
    "    {'use_smote': False, 'pos_weight': 0.952551, 'description': 'Without SMOTE, With pos_weight'},\n",
    "    {'use_smote': False, 'pos_weight': 1.0, 'description': 'Without SMOTE, Without pos_weight'}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for experiment in experiments:\n",
    "    # Build the XGBoost model with or without pos_weight\n",
    "    model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=experiment['pos_weight'])\n",
    "\n",
    "    # Create the pipeline with or without SMOTE\n",
    "    if experiment['use_smote']:\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),  # Assuming `preprocessor` is already defined for your data\n",
    "            ('smote', SMOTE(sampling_strategy=0.35, random_state=42)),\n",
    "            ('model', model)\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = ImbPipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    # Run cross-validation\n",
    "    cv_scores_auprc = cross_val_score(pipeline, X_train, y_train, cv=kf, \n",
    "                                      scoring=make_scorer(auprc_score, needs_proba=True))\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Description': experiment['description'],\n",
    "        'AUPRC Scores': cv_scores_auprc,\n",
    "        'Mean AUPRC': np.mean(cv_scores_auprc)\n",
    "    })\n",
    "\n",
    "# Print results for each experiment\n",
    "for result in results:\n",
    "    print(f\"Experiment: {result['Description']}\")\n",
    "    print(f\"Cross-validated AUPRC Scores: {result['AUPRC Scores']}\")\n",
    "    print(f\"Mean AUPRC Score: {result['Mean AUPRC']:.4f}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble Model\n",
    "\n",
    "Our dataset has high dimensionality which might lead to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9643\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9647\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9644\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9652\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9652\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9639\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9652\n",
      "Testing: SMOTE=None, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9677\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9546\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9564\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9590\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9574\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9596\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9524\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9572\n",
      "Testing: SMOTE=None, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9546\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9563\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9560\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9560\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9556\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9553\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9552\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9556\n",
      "Testing: SMOTE=0.3, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9556\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9377\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9326\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9294\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9425\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9365\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9438\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9456\n",
      "Testing: SMOTE=0.3, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9316\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9549\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9540\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9543\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9542\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9544\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9555\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9559\n",
      "Testing: SMOTE=0.5, final_estimator=LogisticRegression(max_iter=1000), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9555\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9518\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9454\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9433\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=1, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9536\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=1\n",
      "AUPRC: 0.9467\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=0.1, reg_lambda=10\n",
      "AUPRC: 0.9512\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=1\n",
      "AUPRC: 0.9445\n",
      "Testing: SMOTE=0.5, final_estimator=RandomForestClassifier(n_estimators=200, random_state=42), scale_pos_weight=3.92, alpha=1, reg_lambda=10\n",
      "AUPRC: 0.9375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from imblearn.pipeline import Pipeline  # Use imblearn's Pipeline for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Preprocessing pipeline (categorical + numeric)\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "# Function to test different final estimators, hyperparameters, and SMOTE strategies\n",
    "def test_stacking_model(final_estimators, scale_weights, alphas, lambdas, smote_strategies):\n",
    "    for smote_strategy in smote_strategies:\n",
    "        for final_estimator in final_estimators:\n",
    "            for scale_pos_weight in scale_weights:\n",
    "                for alpha in alphas:\n",
    "                    for reg_lambda in lambdas:\n",
    "                        print(f'Testing: SMOTE={smote_strategy}, final_estimator={final_estimator}, scale_pos_weight={scale_pos_weight}, alpha={alpha}, reg_lambda={reg_lambda}')\n",
    "                        \n",
    "                        # Create the XGBoost model with varying hyperparameters\n",
    "                        xgb_model = xgb.XGBClassifier(\n",
    "                            eval_metric='aucpr',\n",
    "                            random_state=42,\n",
    "                            scale_pos_weight=scale_pos_weight,\n",
    "                            alpha=alpha,\n",
    "                            reg_lambda=reg_lambda\n",
    "                        )\n",
    "                        \n",
    "                        # Base models for stacking\n",
    "                        base_models = [\n",
    "                            ('lr', LogisticRegression(max_iter=500)),\n",
    "                            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "                            ('xgb', xgb_model)\n",
    "                        ]\n",
    "                        \n",
    "                        # StackingClassifier with varying final estimator\n",
    "                        stacked_model = StackingClassifier(\n",
    "                            estimators=base_models,\n",
    "                            final_estimator=final_estimator,\n",
    "                            cv=5\n",
    "                        )\n",
    "                        \n",
    "                        # Create the pipeline with or without SMOTE\n",
    "                        if smote_strategy is not None:\n",
    "                            smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "                            pipeline = Pipeline(steps=[\n",
    "                                ('preprocessor', preprocessor),\n",
    "                                ('smote', smote),\n",
    "                                ('model', stacked_model)\n",
    "                            ])\n",
    "                        else:\n",
    "                            pipeline = Pipeline(steps=[\n",
    "                                ('preprocessor', preprocessor),\n",
    "                                ('model', stacked_model)\n",
    "                            ])\n",
    "                        \n",
    "                        # Train the pipeline\n",
    "                        pipeline.fit(X_train, y_train)\n",
    "                        \n",
    "                        # Calculate and print the ROC AUC score\n",
    "                        y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "                        \n",
    "                        # Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "                        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "                        auprc = auc(recall, precision)\n",
    "                        print(f'AUPRC: {auprc:.4f}')\n",
    "\n",
    "# Define the final estimators, scale weights, alphas, lambdas, and SMOTE strategies to test\n",
    "final_estimators = [LogisticRegression(max_iter=1000), RandomForestClassifier(n_estimators=200, random_state=42)]\n",
    "scale_weights = [1, 3.92]\n",
    "alphas = [0.1, 1]\n",
    "lambdas = [1, 10]\n",
    "smote_strategies = [None, 0.30, 0.5]  # None for no SMOTE, 0.35 for 35% minority class\n",
    "\n",
    "# Run the testing function\n",
    "test_stacking_model(final_estimators, scale_weights, alphas, lambdas, smote_strategies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUPRC on Validation Set: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_stacking_model.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, auc, average_precision_score\n",
    "from imblearn.pipeline import Pipeline  # Use imblearn's Pipeline for SMOTE\n",
    "import xgboost as xgb\n",
    "import joblib  # For saving the model\n",
    "\n",
    "# Preprocessing pipeline (categorical + numeric)\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "# Set the best known parameters\n",
    "smote_strategy = None\n",
    "final_estimator = LogisticRegression(max_iter=1000)\n",
    "scale_pos_weight = 3.92\n",
    "alpha = 0.1\n",
    "reg_lambda = 10\n",
    "\n",
    "# Create the XGBoost model with the best known parameters\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    alpha=alpha,\n",
    "    reg_lambda=reg_lambda\n",
    ")\n",
    "\n",
    "# Define base classifiers\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=500)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# StackingClassifier with the best final estimator\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=final_estimator,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create the pipeline without SMOTE (since SMOTE strategy is None)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', stacked_model)\n",
    "])\n",
    "\n",
    "# Train the pipeline with the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set and calculate AUPRC\n",
    "y_pred_proba = pipeline.predict_proba(X_val)[:, 1]  # Get probability for the positive class\n",
    "precision, recall, _ = precision_recall_curve(y_val, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "\n",
    "# Print the AUPRC score\n",
    "print(f'Best AUPRC on Validation Set: {auprc:.4f}')\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(pipeline, 'best_stacking_model.pkl')\n",
    "\n",
    "# Load and use the best model later if needed\n",
    "# best_model = joblib.load('best_stacking_model.pkl')\n",
    "# y_pred_prob = best_model.predict_proba(new_data)[:, 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
