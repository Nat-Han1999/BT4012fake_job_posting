{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model using XGBoost\n",
    "\n",
    "This notebook explores the following techniques to improve the performance of the XGBoost model in detecting fraudulent job postings:\n",
    "\n",
    "- **Various `scale_pos_weight`:** Adjusts the balance between positive (fraudulent) and negative (non-fraudulent) weights to handle the class imbalance in the dataset.\n",
    "\n",
    "- **Threshold Adjustment:** Helps to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent.\n",
    "\n",
    "- **Stratified K-Fold Cross-Validation:** Ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the original dataset, providing a more reliable evaluation.\n",
    "\n",
    "- **Hyperparameter Tuning:** Used to find the optimal set of model parameters (such as learning rate, tree depth, and number of estimators) to improve overall model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data_cleaned.csv') ##need to replace with ur own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>fraudulent</th>\n",
       "      <th>has_location</th>\n",
       "      <th>has_employment_type</th>\n",
       "      <th>has_required_experience</th>\n",
       "      <th>has_required_education</th>\n",
       "      <th>has_industry</th>\n",
       "      <th>has_function</th>\n",
       "      <th>...</th>\n",
       "      <th>city_ wilmington</th>\n",
       "      <th>city_ woodbridge</th>\n",
       "      <th>city_ woodruff</th>\n",
       "      <th>city_ worcester</th>\n",
       "      <th>city_ İstanbul</th>\n",
       "      <th>city_ Αthens</th>\n",
       "      <th>city_ Αθήνα</th>\n",
       "      <th>city_ ΕΛΛΗΝΙΚΟ</th>\n",
       "      <th>city_ 마포구 동교동</th>\n",
       "      <th>city_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14617 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   telecommuting  has_company_logo  has_questions  fraudulent  has_location  \\\n",
       "0              0                 1              0           0             1   \n",
       "1              0                 1              0           0             1   \n",
       "2              0                 1              0           0             1   \n",
       "3              0                 1              0           0             1   \n",
       "4              0                 1              1           0             1   \n",
       "\n",
       "   has_employment_type  has_required_experience  has_required_education  \\\n",
       "0                    1                        1                       0   \n",
       "1                    1                        1                       0   \n",
       "2                    0                        0                       0   \n",
       "3                    1                        1                       1   \n",
       "4                    1                        1                       1   \n",
       "\n",
       "   has_industry  has_function  ...  city_ wilmington  city_ woodbridge   \\\n",
       "0             0             1  ...               0.0                0.0   \n",
       "1             1             1  ...               0.0                0.0   \n",
       "2             0             0  ...               0.0                0.0   \n",
       "3             1             1  ...               0.0                0.0   \n",
       "4             1             1  ...               0.0                0.0   \n",
       "\n",
       "   city_ woodruff  city_ worcester  city_ İstanbul  city_ Αthens  city_ Αθήνα  \\\n",
       "0             0.0              0.0             0.0           0.0          0.0   \n",
       "1             0.0              0.0             0.0           0.0          0.0   \n",
       "2             0.0              0.0             0.0           0.0          0.0   \n",
       "3             0.0              0.0             0.0           0.0          0.0   \n",
       "4             0.0              0.0             0.0           0.0          0.0   \n",
       "\n",
       "   city_ ΕΛΛΗΝΙΚΟ  city_ 마포구 동교동  city_Unknown  \n",
       "0             0.0            0.0           0.0  \n",
       "1             0.0            0.0           0.0  \n",
       "2             0.0            0.0           0.0  \n",
       "3             0.0            0.0           0.0  \n",
       "4             0.0            0.0           0.0  \n",
       "\n",
       "[5 rows x 14617 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = list(df.select_dtypes(include=['object']).columns)\n",
    "numeric_features = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "if 'fraudulent' in numeric_features:\n",
    "    numeric_features.remove('fraudulent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(categorical_features, numeric_features):\n",
    "    transformers = []\n",
    "\n",
    "    transformers.append(\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    )\n",
    "\n",
    "    transformers.append(('scaler', StandardScaler(), numeric_features))\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "X = pd.concat([df[categorical_features + numeric_features]], axis=1)\n",
    "y = df['fraudulent']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base XGBoost Model\n",
    "\n",
    "A base XGBoost model is used as a benchmark to identify further techniques needed to be applied to the dataset.\n",
    "\n",
    "Run time: 2.5 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9907    0.9988    0.9947      3403\n",
      "           1     0.9724    0.8150    0.8868       173\n",
      "\n",
      "    accuracy                         0.9899      3576\n",
      "   macro avg     0.9815    0.9069    0.9408      3576\n",
      "weighted avg     0.9898    0.9899    0.9895      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3399    4]\n",
      " [  32  141]]\n",
      "ROC AUC Score: 0.9937\n",
      "AUPRC: 0.9507\n"
     ]
    }
   ],
   "source": [
    "# Build the XGBoost model\n",
    "model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting scale_pos_weight in XGBoost\n",
    "\n",
    "Since scale_pos_weight is used to control the balance of positive and negative weights, which is useful for imbalanced datasets. Setting this parameter will help the algorithm give more weights to the minority class during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing scale_pos_weight = 19.64\n",
      "Confusion Matrix:\n",
      "[[3383   20]\n",
      " [  19  154]]\n",
      "ROC AUC Score: 0.9926\n",
      "AUPRC: 0.9431\n",
      "\n",
      "Testing scale_pos_weight = 58.92\n",
      "Confusion Matrix:\n",
      "[[3365   38]\n",
      " [  25  148]]\n",
      "ROC AUC Score: 0.9898\n",
      "AUPRC: 0.9283\n",
      "\n",
      "Testing scale_pos_weight = 98.20\n",
      "Confusion Matrix:\n",
      "[[3359   44]\n",
      " [  19  154]]\n",
      "ROC AUC Score: 0.9900\n",
      "AUPRC: 0.9287\n",
      "\n",
      "Testing scale_pos_weight = 6.55\n",
      "Confusion Matrix:\n",
      "[[3394    9]\n",
      " [  24  149]]\n",
      "ROC AUC Score: 0.9939\n",
      "AUPRC: 0.9526\n",
      "\n",
      "Testing scale_pos_weight = 3.93\n",
      "Confusion Matrix:\n",
      "[[3397    6]\n",
      " [  29  144]]\n",
      "ROC AUC Score: 0.9935\n",
      "AUPRC: 0.9431\n",
      "\n",
      "Evaluation Metrics for Different scale_pos_weight Values:\n",
      "   scale_pos_weight   roc_auc     auprc\n",
      "0         19.640693  0.992577  0.943083\n",
      "1         58.922078  0.989803  0.928315\n",
      "2         98.203463  0.989987  0.928705\n",
      "3          6.546898  0.993882  0.952600\n",
      "4          3.928139  0.993520  0.943123\n",
      "\n",
      "Best scale_pos_weight based on AUPRC: 6.55 with AUPRC: 0.9526\n"
     ]
    }
   ],
   "source": [
    "# Define scale_pos_weight values to test\n",
    "counter = Counter(y_train)\n",
    "scale_pos_weight_base = counter[0] / counter[1]\n",
    "\n",
    "scale_pos_weights = [scale_pos_weight_base, scale_pos_weight_base * 3, scale_pos_weight_base * 5, scale_pos_weight_base / 3, scale_pos_weight_base / 5]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate over different scale_pos_weight values\n",
    "for spw in scale_pos_weights:\n",
    "    print(f\"\\nTesting scale_pos_weight = {spw:.2f}\")\n",
    "    \n",
    "    # Build the XGBoost model with the current scale_pos_weight\n",
    "    model = xgb.XGBClassifier(eval_metric='aucpr', random_state=42, scale_pos_weight=spw)\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate and print the ROC AUC score\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "    \n",
    "    # Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    auprc = auc(recall, precision)\n",
    "    print(f'AUPRC: {auprc:.4f}')\n",
    "    \n",
    "    # Store the result\n",
    "    results.append({\n",
    "        'scale_pos_weight': spw,\n",
    "        'roc_auc': roc_auc,\n",
    "        'auprc': auprc\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nEvaluation Metrics for Different scale_pos_weight Values:\")\n",
    "print(df_results)\n",
    "\n",
    "# Find the best scale_pos_weight based on AUPRC\n",
    "best_spw = df_results.loc[df_results['auprc'].idxmax()]\n",
    "print(f\"\\nBest scale_pos_weight based on AUPRC: {best_spw['scale_pos_weight']:.2f} with AUPRC: {best_spw['auprc']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using different weights for the classes resulted in an improvement in AUPRC. Therefore, we will select the weight with the highest AUPRC, which is scale_pos_weight = 6.55."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Ensemble Model\n",
    "\n",
    "Our dataset has high dimensionality which might lead to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9930    1.0000    0.9965      3403\n",
      "           1     1.0000    0.8613    0.9255       173\n",
      "\n",
      "    accuracy                         0.9933      3576\n",
      "   macro avg     0.9965    0.9306    0.9610      3576\n",
      "weighted avg     0.9933    0.9933    0.9931      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3403    0]\n",
      " [  24  149]]\n",
      "ROC AUC Score: 0.9968\n",
      "AUPRC: 0.9735\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "import xgboost as xgb\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Preprocessing pipeline (categorical + numeric)\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "### Step 1: XGBoost with Regularization ###\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=6.55,  # Adjust if necessary based on class imbalance\n",
    "    alpha=1,  # L1 regularization (try different values)\n",
    "    reg_lambda=10  # L2 regularization (try different values)\n",
    ")\n",
    "\n",
    "### Step 2: Stacking Ensemble Model ###\n",
    "# Base models for stacking\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=500)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# StackingClassifier with Logistic Regression as the final estimator\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and stacking\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', stacked_model)\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 13611, 1: 693})\n",
      "Resampled class distribution: Counter({0: 2100, 1: 693})\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    0.9880    0.9914      3403\n",
      "           1     0.7919    0.9017    0.8432       173\n",
      "\n",
      "    accuracy                         0.9838      3576\n",
      "   macro avg     0.8934    0.9448    0.9173      3576\n",
      "weighted avg     0.9851    0.9838    0.9843      3576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3362   41]\n",
      " [  17  156]]\n",
      "ROC AUC Score: 0.9946\n",
      "AUPRC: 0.9500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "import xgboost as xgb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the class distribution before undersampling\n",
    "print(f'Original class distribution: {Counter(y_train)}')\n",
    "\n",
    "# Undersample the majority class using RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler(sampling_strategy=0.33, random_state=42)  # Undersample to a 1:3 ratio\n",
    "X_train_resampled, y_train_resampled = under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after undersampling\n",
    "print(f'Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "# Preprocessing pipeline (categorical + numeric)\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "### Step 1: XGBoost with Regularization (without pos_weight) ###\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=6.55,  # Adjust if necessary based on class imbalance\n",
    "    alpha=1,  # L1 regularization (try different values)\n",
    "    reg_lambda=10  # L2 regularization (try different values)\n",
    ")\n",
    "\n",
    "### Step 2: Stacking Ensemble Model ###\n",
    "# Base models for stacking\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# StackingClassifier with Logistic Regression as the final estimator\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and stacking\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', stacked_model)\n",
    "])\n",
    "\n",
    "# Train the pipeline on the undersampled data\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 13611, 1: 693})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "SMOTE-NC is not designed to work only with numerical features. It requires some categorical features.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Apply SMOTENC for oversampling\u001b[39;00m\n\u001b[1;32m     24\u001b[0m smote_nc \u001b[38;5;241m=\u001b[39m SMOTENC(categorical_features\u001b[38;5;241m=\u001b[39mcategorical_feature_indices, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote_nc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Check the class distribution after SMOTENC\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResampled class distribution: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_train_resampled)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/over_sampling/_smote/base.py:654\u001b[0m, in \u001b[0;36mSMOTENC._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_ \u001b[38;5;241m=\u001b[39m _num_features(X)\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_types(X)\n\u001b[0;32m--> 654\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m X_continuous \u001b[38;5;241m=\u001b[39m _safe_indexing(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuous_features_, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    657\u001b[0m X_continuous \u001b[38;5;241m=\u001b[39m check_array(X_continuous, accept_sparse\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/imblearn/over_sampling/_smote/base.py:637\u001b[0m, in \u001b[0;36mSMOTENC._validate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTE-NC is not designed to work only with categorical \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures. It requires some numerical features.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_features_\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTE-NC is not designed to work only with numerical \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures. It requires some categorical features.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: SMOTE-NC is not designed to work only with numerical features. It requires some categorical features."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve, auc\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from collections import Counter\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Check the class distribution before SMOTENC\n",
    "print(f'Original class distribution: {Counter(y_train)}')\n",
    "\n",
    "# Specify categorical feature indices for SMOTENC (based on your dataset structure)\n",
    "categorical_feature_indices = [X.columns.get_loc(col) for col in categorical_features]\n",
    "\n",
    "# Apply SMOTENC for oversampling\n",
    "smote_nc = SMOTENC(categorical_features=categorical_feature_indices, random_state=42, sampling_strategy=0.3)\n",
    "X_train_resampled, y_train_resampled = smote_nc.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTENC\n",
    "print(f'Resampled class distribution: {Counter(y_train_resampled)}')\n",
    "\n",
    "# Preprocessing pipeline (categorical + numeric)\n",
    "preprocessor = create_preprocessor(categorical_features, numeric_features)\n",
    "\n",
    "### Step 1: XGBoost with Regularization (without pos_weight) ###\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=6.546898, \n",
    "    alpha=1,  # L1 regularization (try different values)\n",
    "    lambda_=10  # L2 regularization (try different values)\n",
    ")\n",
    "\n",
    "### Step 2: Stacking Ensemble Model ###\n",
    "# Base models for stacking\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb_model)\n",
    "]\n",
    "\n",
    "# StackingClassifier with Logistic Regression as the final estimator\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and stacking\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', stacked_model)\n",
    "])\n",
    "\n",
    "# Train the pipeline on the resampled data\n",
    "pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "\n",
    "# Calculate and print the ROC AUC score\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Get probability for the positive class\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f'ROC AUC Score: {roc_auc:.4f}')\n",
    "\n",
    "# Calculate and print the AUPRC (Area Under Precision-Recall Curve)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering a range of scale_pos_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In imbalanced datasets like ours, experimenting with a range of scale_pos_weight values helps us balance the importance between the minority (fraudulent jobs) and majority (non-fraudulent jobs) classes, ensuring that the model doesn't become biased towards the majority class and improves its ability to detect fraudulent jobs (i.e., higher recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Classification Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above experiement, we observed that the peak in AUCPR at approximately 0.9309 when scale_pos_weight is 23.57.\n",
    "\n",
    "Another possible action is to adjsut the classification threshold. Instead of using the default probability threshold of 0.5, we can experiment with different thresholds to balance precision and recall. This would help to balance precision and recall by modifying the cutoff point at which a job posting is classified as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.3, Precision: 0.6653, Recall: 0.9422, F1-Score: 0.7799\n",
      "Threshold: 0.4, Precision: 0.7285, Recall: 0.9306, F1-Score: 0.8173\n",
      "Threshold: 0.5, Precision: 0.7919, Recall: 0.9017, F1-Score: 0.8432\n",
      "Threshold: 0.6, Precision: 0.8432, Recall: 0.9017, F1-Score: 0.8715\n",
      "Threshold: 0.7, Precision: 0.8988, Recall: 0.8728, F1-Score: 0.8856\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for threshold in thresholds:\n",
    "    y_pred_adjusted = (y_pred_proba >= threshold).astype(int)\n",
    "    report = classification_report(y_test, y_pred_adjusted, output_dict=True, digits=4)\n",
    "    precision = report['1']['precision']\n",
    "    recall = report['1']['recall']\n",
    "    f1_score = report['1']['f1-score']\n",
    "    print(f\"Threshold: {threshold}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiement on threshold above, we can see that as threshold increases, precision increases which means that fewer non-fraudulent jobs are incorrectly classified as fraudulent.\n",
    "\n",
    "Recall however, decreases which means that more fraudulent jobs are missed.\n",
    "\n",
    "As we need to find a balance between precision and recall, assuming that the risk and reward is equal, we will identify the point where the F1-Score peaks, which is when the threshold is 0.7.\n",
    "\n",
    "Area under the precision-recall curve is not used here as it is independent of the threshold number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration both of our findings from the scale_pos_weight and threshold experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report at Threshold 0.5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9950    0.9885    0.9917      3403\n",
      "           1     0.8000    0.9017    0.8478       173\n",
      "\n",
      "    accuracy                         0.9843      3576\n",
      "   macro avg     0.8975    0.9451    0.9198      3576\n",
      "weighted avg     0.9855    0.9843    0.9848      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.5:\n",
      "[[3364   39]\n",
      " [  17  156]]\n",
      "AUPRC: 0.9309\n",
      "Classification Report at Threshold 0.7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9918    0.9953    0.9935      3403\n",
      "           1     0.9006    0.8382    0.8683       173\n",
      "\n",
      "    accuracy                         0.9877      3576\n",
      "   macro avg     0.9462    0.9167    0.9309      3576\n",
      "weighted avg     0.9874    0.9877    0.9875      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.7:\n",
      "[[3387   16]\n",
      " [  28  145]]\n",
      "AUPRC: 0.9309\n"
     ]
    }
   ],
   "source": [
    "# Optimal scale_pos_weight\n",
    "optimal_scale_pos_weight = 23.57\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=optimal_scale_pos_weight,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Previous threshold of 0.5\n",
    "optimal_threshold = 0.5\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Classification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')  \n",
    "\n",
    "# Optimal threshold of 0.7\n",
    "optimal_threshold = 0.7\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Classification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n",
    "\n",
    "y_pred_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "auprc = auc(recall, precision)\n",
    "print(f'AUPRC: {auprc:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the optimal threshold improves the f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using StratifiedKFold ensures that each fold in cross-validation maintains the same proportion of fraudulent and non-fraudulent jobs as in the entire dataset. Doing so will help provide more reliable evaluation for our highly imbalanced data and preventing bias towards the majority class during model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Scale Pos Weight for Fold 1: 6.55\n",
      "AUPRC for Fold 1: 0.9414\n",
      "\n",
      "Fold 2/5\n",
      "Scale Pos Weight for Fold 2: 6.55\n",
      "AUPRC for Fold 2: 0.9286\n",
      "\n",
      "Fold 3/5\n",
      "Scale Pos Weight for Fold 3: 6.55\n",
      "AUPRC for Fold 3: 0.9340\n",
      "\n",
      "Fold 4/5\n",
      "Scale Pos Weight for Fold 4: 6.55\n",
      "AUPRC for Fold 4: 0.9570\n",
      "\n",
      "Fold 5/5\n",
      "Scale Pos Weight for Fold 5: 6.55\n",
      "AUPRC for Fold 5: 0.9143\n",
      "\n",
      "Average AUPRC across all folds: 0.9350\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "aucpr_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    \n",
    "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    scale_pos_weight_fold = 6.546898\n",
    "    print(f\"Scale Pos Weight for Fold {fold + 1}: {scale_pos_weight_fold:.2f}\")\n",
    "    \n",
    "    model_fold = xgb.XGBClassifier(\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight_fold\n",
    "    )\n",
    "    \n",
    "    pipeline_fold = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model_fold)\n",
    "    ])\n",
    "    \n",
    "    pipeline_fold.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    y_pred_proba_fold = pipeline_fold.predict_proba(X_test_fold)[:, 1]\n",
    "    \n",
    "    # Calculate Precision-Recall curve and AUPRC\n",
    "    precision, recall, _ = precision_recall_curve(y_test_fold, y_pred_proba_fold)\n",
    "    aucpr = auc(recall, precision)\n",
    "    aucpr_scores.append(aucpr)\n",
    "    \n",
    "    print(f\"AUPRC for Fold {fold + 1}: {aucpr:.4f}\")\n",
    "\n",
    "# Calculate and print average AUPRC across all folds\n",
    "avg_aucpr = np.mean(aucpr_scores)\n",
    "print(f\"\\nAverage AUPRC across all folds: {avg_aucpr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SMOTE for imbalanced dataset\n",
    "\n",
    "We are considering to use SMOTE for our dataset. However, our data is high-dimensional because of the TF-IDF. SMOTE may not be as effective in such spaces because it relies on computing nearest neighbors, which can be unreliable in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "In this section, we intend to try out the different hyperparameters to test out if other parameters could improve our AUPRC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of code takes 4 hours to run. Please take note before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Scale Pos Weight: 23.57\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Parameters:\n",
      "{'model__n_estimators': 300, 'model__max_depth': 7, 'model__learning_rate': 0.1, 'model__gamma': 0.1}\n",
      "Best AUPRC Score from Cross-Validation: 0.9117\n",
      "\n",
      "Classification Report at Threshold 0.7:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9962    0.9936      3403\n",
      "           1     0.9161    0.8208    0.8659       173\n",
      "\n",
      "    accuracy                         0.9877      3576\n",
      "   macro avg     0.9535    0.9085    0.9297      3576\n",
      "weighted avg     0.9873    0.9877    0.9874      3576\n",
      "\n",
      "Confusion Matrix at Threshold 0.7:\n",
      "[[3390   13]\n",
      " [  31  142]]\n"
     ]
    }
   ],
   "source": [
    "scale_pos_weight = 23.57\n",
    "print(f\"Global Scale Pos Weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__gamma': [0, 0.1, 0.3],\n",
    "}\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='aucpr',  \n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Custom scoring function to calculate AUPRC\n",
    "def custom_auprc(estimator, X, y_true):\n",
    "    y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "# Use custom_auprc without make_scorer (direct function)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring=custom_auprc,  # Custom AUPRC function without make_scorer\n",
    "    cv=skf,\n",
    "    n_jobs=1,  # Run without parallelization to avoid pickling issues\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best AUPRC Score from Cross-Validation: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Use the best pipeline after hyperparameter tuning\n",
    "best_pipeline = random_search.best_estimator_\n",
    "\n",
    "# Apply the optimal threshold for final predictions\n",
    "optimal_threshold = 0.7\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_optimal = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(f\"\\nClassification Report at Threshold {optimal_threshold}:\")\n",
    "print(classification_report(y_test, y_pred_optimal, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_optimal)\n",
    "print(f\"Confusion Matrix at Threshold {optimal_threshold}:\")\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
